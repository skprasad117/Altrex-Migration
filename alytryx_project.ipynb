{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f32104",
   "metadata": {},
   "source": [
    "## Reading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "79151376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "492b9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "04598003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e18d5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, regexp_replace, sum, lit, max, min, first,to_timestamp, trim, upper,to_date, to_timestamp, date_format, when, input_file_name\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, DateType, BooleanType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "1243488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "PandasDataFrame = TypeVar('pandas.core.frame.DataFrame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "acf9d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master(\"local[*]\").appName(\"read_csv\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"read_csv\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f5c4c0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-E2PD0VIH:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>read_csv</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24afd9d7090>"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e90ff3",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07760e5",
   "metadata": {},
   "source": [
    "### Duplicate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "275d5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dub_column(df:PandasDataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Count the occurrences of each column name in a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (PandasDataFrame): The Pandas DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are column names, and the values represent\n",
    "        the number of occurrences of each column name in the DataFrame.\n",
    "    \"\"\"\n",
    "    col = dict()\n",
    "    res = list()\n",
    "    for column in df.columns:\n",
    "        if column in col:\n",
    "            col[column] +=1\n",
    "        else:\n",
    "            col[column]=1\n",
    "#     for column in col:\n",
    "#         if col[column]>2:\n",
    "            \n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed32742",
   "metadata": {},
   "source": [
    "### show and remove duplicate row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "6c957d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show and remove duplicate row\n",
    "def remove_dup_rows(df, show= True):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a Pandas DataFrame and optionally display them.\n",
    "\n",
    "    Parameters:\n",
    "        df (PandasDataFrame): The Pandas DataFrame from which duplicate rows will be removed.\n",
    "        show (bool, optional): If True, display the duplicate rows before removal. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        PandasDataFrame: A DataFrame with duplicate rows removed.\n",
    "    \"\"\"\n",
    "    df_grou = df.groupby(*df.columns).count()\n",
    "    dup = df_grou.filter(col(\"count\")>1)\n",
    "    if show == True:\n",
    "        print(dup.show())\n",
    "#     df = df.drop_duplicates()\n",
    "    return df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ecd8d",
   "metadata": {},
   "source": [
    "## some funcction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "585c9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a UDF to parse the date string\n",
    "@udf(DateType())\n",
    "def parse_date(date_str):\n",
    "    \"\"\"\n",
    "    Parse a date string in the format MM/DD/YYYY to a DateType in PySpark.\n",
    "\n",
    "    This User-Defined Function (UDF) takes a date string in the format \"MM/DD/YYYY\" and converts it\n",
    "    into a DateType object in PySpark.\n",
    "\n",
    "    Parameters:\n",
    "        date_str (str): A date string in the format \"MM/DD/YYYY\" to be parsed.\n",
    "\n",
    "    Returns:\n",
    "        DateType: A DateType object representing the parsed date, or None if the input string\n",
    "        is not in the expected format.\"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%m/%d/%Y\").date()\n",
    "    except ValueError:\n",
    "        return None  # Handle invalid date strings as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69ce0d",
   "metadata": {},
   "source": [
    "### Change datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "8a913a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype(dataframe: PandasDataFrame, column_name: str,change_to: type(\"Type\"))->PandasDataFrame:\n",
    "    '''usage: change_dtype(dataframe, column_name,change_to)\n",
    "       return : dataframe.withColumn(column_name, col(column_name).cast(change_to))'''\n",
    "    return dataframe.withColumn(column_name, col(column_name).cast(change_to))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2cfcd9",
   "metadata": {},
   "source": [
    "### remove illegal character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "443fb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_illegal_char(dataframe: PandasDataFrame,column:list)-> PandasDataFrame:\n",
    "    \"\"\"\n",
    "    Remove specified illegal characters from a column in a PySpark DataFrame.\n",
    "\n",
    "    This function takes a PySpark DataFrame and a column name, and it removes\n",
    "    specified illegal characters from the values in the specified column.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (DataFrame): The PySpark DataFrame to process.\n",
    "        column_name (str): The name of the column from which illegal characters will be removed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame with illegal characters removed from the specified column.\"\"\"\n",
    "    \n",
    "    illegal_character = ['Can','\\$',',',\"\\(\",\"\\)\"]\n",
    "    for column_name in column:\n",
    "        for character in illegal_character:\n",
    "            dataframe = dataframe.withColumn(column_name, regexp_replace(col(column_name), character, ''))\n",
    "    #                              .withColumn('Open Balance', regexp_replace(col('Open Balance'), 'Can', ''))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "850380fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_col(dataframe:PandasDataFrame, column_name: str)-> PandasDataFrame:\n",
    "    try:\n",
    "        return dataframe.withColumn(column_name, trim(col(column_name)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "33dcbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanining data and creating csv file to avoid multiline issue\n",
    "try:\n",
    "    filepath = \"D:/Bluethink/altryx/AL/Contract Terms per Client (1).xlsx\"\n",
    "    filename = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    pandas_df = pd.read_excel(filepath)\n",
    "    df= pandas_df\n",
    "    for column in df.columns:\n",
    "#         print(column, \"datatype : \",pandas_df[column].dtypes, end=\" : -->\")\n",
    "        datatype = df[column].dtypes\n",
    "        df[column] = df[column].astype(str)\n",
    "#         print(df[column].dtypes, end=\" -->\")\n",
    "        df.fillna(\"\", inplace=True)\n",
    "        df[column] = df[column].str.replace('\\n', ' ')\n",
    "        df[column] = df[column].astype(datatype)\n",
    "#         print(df[column].dtypes)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    new_file = \"csv_converted/temp\"+\"/\" +filename+\".csv\"\n",
    "    df.to_csv(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "98cc6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_change_dtype(dataframe : PandasDataFrame, altryx_datatype: dict, datetimedata:list)->PandasDataFrame:\n",
    "    \"\"\"\n",
    "    Change the data types of columns in a Pandas DataFrame based on a provided mapping.\n",
    "\n",
    "    This function converts the data types of columns in a Pandas DataFrame according to a\n",
    "    provided mapping of Alteryx data types to Python data types.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (PandasDataFrame): The Pandas DataFrame to modify.\n",
    "        altryx_datatype (dict): A dictionary mapping column names to Alteryx data types.\n",
    "\n",
    "    Returns:\n",
    "        PandasDataFrame: The modified DataFrame with updated column data types.\"\"\"\n",
    "    \n",
    "    alteryx_to_python_data_types = {\n",
    "    \"Boolean\": BooleanType(),\n",
    "    \"Byte\": IntegerType(),\n",
    "    \"Date\": DateType(),\n",
    "    \"DateTime\": DateType(),\n",
    "    \"Decimal\": FloatType(),\n",
    "    \"Double\": FloatType(),\n",
    "    \"FixedDecimal\": FloatType(),\n",
    "    \"Float\": FloatType(),\n",
    "    \"Int16\": IntegerType(),\n",
    "    \"Int32\": IntegerType(),\n",
    "    \"Int64\": IntegerType(),\n",
    "    \"String\": StringType(),\n",
    "    \"Time\": TimestampType(),\n",
    "    \"V_String\": StringType(),\n",
    "    \"V_WString\": StringType(),\n",
    "    \"WString\": StringType(),\n",
    "    }\n",
    "    for column in dataframe.columns:\n",
    "        if (\"date\" in column) | (\"Date\" in column):\n",
    "            if column in datetimedata:\n",
    "                dataframe =dataframe.withColumn(column, to_timestamp(dataframe[column],\"M/d/yyyy HH:mm\"))\n",
    "            else:\n",
    "                dataframe =dataframe.withColumn(column, to_date(dataframe[column],'M/d/yyyy'))\n",
    "        else:\n",
    "            dataframe = dataframe.withColumn(column, dataframe[column].cast(alteryx_to_python_data_types[altryx_datatype[column]]))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f694a",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae90b7",
   "metadata": {},
   "source": [
    "# To load file correctly\n",
    "- load using csv file\n",
    "- add location of dile as 'FileName' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e5d4f",
   "metadata": {},
   "source": [
    "### Reading file: csv  no 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ff12abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "sales_report_kmpg_july=spark.read.options(inferSchema='True').csv('csv_converted\\Scribe_amreica.csv', header=True, inferSchema=True, sep=',')\n",
    "sales_report_kmpg_july=sales_report_kmpg_july.withColumn(\"FileName\", input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d601aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_altryx_data_type = {\n",
    "    \"Transaction ID\": \"V_String\",\n",
    "    \"Document Number\": \"V_String\",\n",
    "    \"Transaction Type\": \"V_String\",\n",
    "    \"Project ID\": \"Double\",\n",
    "    \"Customer\": \"V_WString\",\n",
    "    \"Terms\": \"V_String\",\n",
    "    \"Currency\": \"V_String\",\n",
    "    \"Gross Amount\": \"Double\",\n",
    "    \"Payment\": \"Double\",\n",
    "    \"Open Balance\": \"Double\",\n",
    "    \"Transaction Create Date\": \"V_String\",\n",
    "    \"Document Date\": \"V_String\",\n",
    "    \"Due Date\": \"V_String\",\n",
    "    \"Days Open\": \"Double\",\n",
    "    \"Service Dates\": \"V_String\",\n",
    "    \"Payment Receipt Date\": \"V_String\",\n",
    "    \"FileName\": \"V_WString\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "6ef1fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_report_kmpg_july = remove_illegal_char(sales_report_kmpg_july,column=['Currency','Gross Amount','Payment','Open Balance',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "93c67f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Create Date datetime\n"
     ]
    }
   ],
   "source": [
    "sales_report_kmpg_july = df_change_dtype(sales_report_kmpg_july,altryx_data_type, datetimedata=['Transaction Create Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "929de585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Transaction Create Date', 'timestamp')]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_report_kmpg_july.select(['Transaction Create Date']).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "8d7137c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Transaction ID', 'string'),\n",
       " ('Document Number', 'string'),\n",
       " ('Transaction Type', 'string'),\n",
       " ('Project ID', 'float'),\n",
       " ('Customer', 'string'),\n",
       " ('Terms', 'string'),\n",
       " ('Currency', 'string'),\n",
       " ('Gross Amount', 'float'),\n",
       " ('Payment', 'float'),\n",
       " ('Open Balance', 'float'),\n",
       " ('Transaction Create Date', 'timestamp'),\n",
       " ('Document Date', 'date'),\n",
       " ('Due Date', 'date'),\n",
       " ('Days Open', 'float'),\n",
       " ('Service Dates', 'date'),\n",
       " ('Payment Receipt Date', 'date'),\n",
       " ('FileName', 'string')]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_report_kmpg_july.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "eb491a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Transaction Create Date', 'date')]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['Transaction Create Date']).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7976435",
   "metadata": {},
   "source": [
    "## Cleaning and Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "415f4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = ['Transaction ID','Document Number','Transaction Type', 'Project ID','Customer','Terms','Currency']\n",
    "numeric_columns = ['Gross Amount','Payment','Open Balance','Days Open']\n",
    "date_columns = ['Transaction Create Date','Document Date','Due Date','Service Dates','Payment Receipt Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1a7bb",
   "metadata": {},
   "source": [
    "### Trim column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c90cc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_report_kmpg_july.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a329cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_report_kmpg_july = trim_col(sales_report_kmpg_july,column_name='Open Balance')\n",
    "for column in sales_report_kmpg_july.columns:\n",
    "    sales_report_kmpg_july = trim_col(sales_report_kmpg_july,column_name=column)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cca84",
   "metadata": {},
   "source": [
    "### Remove illegal characters from numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "232662ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_report_kmpg_july = remove_illegal_char(sales_report_kmpg_july,column_name='Open Balance')\n",
    "for column in numeric_columns:\n",
    "#     print(column)\n",
    "    sales_report_kmpg_july = remove_illegal_char(sales_report_kmpg_july,column_name=column)\n",
    "#     print(sales_report_kmpg_july.show(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1d675",
   "metadata": {},
   "source": [
    "### Change datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aafb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_report_kmpg_july = sales_report_kmpg_july.withColumn('Open Balance', col('Open Balance').cast('float'))\n",
    "# for column in date_columns:\n",
    "#     sales_report_kmpg_july = sales_report_kmpg_july.withColumn(column, col(column).cast(to_timestamp(column)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c4add4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Transaction ID', 'string'),\n",
       " ('Document Number', 'string'),\n",
       " ('Transaction Type', 'string'),\n",
       " ('Project ID', 'string'),\n",
       " ('Customer', 'string'),\n",
       " ('Terms', 'string'),\n",
       " ('Currency', 'string'),\n",
       " ('Gross Amount', 'string'),\n",
       " ('Payment', 'string'),\n",
       " ('Open Balance', 'string'),\n",
       " ('Transaction Create Date', 'string'),\n",
       " ('Document Date', 'string'),\n",
       " ('Due Date', 'string'),\n",
       " ('Days Open', 'string'),\n",
       " ('Service Dates', 'string'),\n",
       " ('Payment Receipt Date', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_report_kmpg_july.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f68ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+----------------+----------+--------------------+-----+--------+------------+-------+------------+-----------------------+-------------+--------+---------+--------------------+--------------------+\n",
      "|Transaction ID|Document Number|Transaction Type|Project ID|            Customer|Terms|Currency|Gross Amount|Payment|Open Balance|Transaction Create Date|Document Date|Due Date|Days Open|       Service Dates|Payment Receipt Date|\n",
      "+--------------+---------------+----------------+----------+--------------------+-----+--------+------------+-------+------------+-----------------------+-------------+--------+---------+--------------------+--------------------+\n",
      "|       3190973|      CUMC15527|     Credit Memo|   1277343|CUMC : Columbia U...| null|     USA|      145.83| 145.83|        0.00|          7/31/23 11:02|    7/31/2023|    null|        0|07/01/2023 to 07/...|           7/30/2023|\n",
      "+--------------+---------------+----------------+----------+--------------------+-----+--------+------------+-------+------------+-----------------------+-------------+--------+---------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_report_kmpg_july.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63924c4",
   "metadata": {},
   "source": [
    "## Operation 1:\n",
    "- 3 lines\n",
    "    - 1. Open Balance != 0----------------------output ---> temp_df , Sum_Open_Balance\n",
    "    - 2. Tranasaction Type  == \"Credit Memo\"----output ----> process_df_result\n",
    "    - 3. Tranasaction Type  != \"Credit Memo\"----output-----> temp_df_process_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c3bf7",
   "metadata": {},
   "source": [
    "### 1. Open Balance != 0 \n",
    "    - filtering the recored `Open Balance !=0`\n",
    "    - Adding Column `Outstanding = \"Open\"`\n",
    "    - Sum of `Open Balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81e16cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of records 3948\n",
      "sum : 29295611.528552473, rows: 3948, columns 19\n"
     ]
    }
   ],
   "source": [
    "sales_report_kmpg_july.filter(sales_report_kmpg_july['Open Balance'] != '0.00').count()\n",
    "temp_df = sales_report_kmpg_july.filter(sales_report_kmpg_july['Open Balance'] != '0.00')\n",
    "print(\"no of records\",temp_df.count())\n",
    "# df_spark.withColumn('year after 2', df_spark['year']+1000).show() refrence\n",
    "\n",
    "# adding column Outstanding = open\n",
    "temp_df = temp_df.withColumn('Outstanding', lit('Open'))\n",
    "temp_df = temp_df.withColumn('Applying Link Amount', temp_df[\"Open Balance\"])\n",
    "temp_df = temp_df.withColumn(\"FileName\", lit(\"saleReprtKmpg_july\"))\n",
    "temp_df = change_dtype(temp_df, column_name='Open Balance',change_to=FloatType())\n",
    "Sum_Open_Balance = temp_df.select(sum(col('Open Balance'))).collect()[0][0]\n",
    "print(f\"sum : {Sum_Open_Balance}, rows: {temp_df.count()}, columns {len(temp_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a885c",
   "metadata": {},
   "source": [
    "### 2. Tranasaction Type  = \"Credit Memo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ff39a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Parameters:  15459\n"
     ]
    }
   ],
   "source": [
    "temp_df_process_2=sales_report_kmpg_july.filter(sales_report_kmpg_july['Transaction Type'] == 'Credit Memo')\n",
    "print(\"Input Parameters: \", temp_df_process_2.count())\n",
    "temp_df_process_2 = temp_df_process_2.withColumn('Gross Amount', col('Gross Amount').cast('float'))\n",
    "temp_df_process_2 = temp_df_process_2.withColumn('Payment', col('Payment').cast('float'))\n",
    "temp_df_process_2 = temp_df_process_2.withColumn('Days Open', col('Days Open').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca77f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_df_process_2 =temp_df_process_2.groupBy(['Transaction ID',\"Document Number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "104b74a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Transaction ID', 'string'),\n",
       " ('Document Number', 'string'),\n",
       " ('Transaction Type', 'string'),\n",
       " ('Project ID', 'string'),\n",
       " ('Customer', 'string'),\n",
       " ('Terms', 'string'),\n",
       " ('Currency', 'string'),\n",
       " ('Gross Amount', 'float'),\n",
       " ('Payment', 'float'),\n",
       " ('Open Balance', 'string'),\n",
       " ('Transaction Create Date', 'string'),\n",
       " ('Document Date', 'string'),\n",
       " ('Due Date', 'string'),\n",
       " ('Days Open', 'float'),\n",
       " ('Service Dates', 'string'),\n",
       " ('Payment Receipt Date', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df_process_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be9ed241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"Transaction Type\": first(col(\"Transaction Type\")),  # Pick the first occurrence of \"Payment Type\"\n",
    "#     \"Project ID\": max(col(\"Profit\"))  # Calculate the maximum profit\n",
    "#     \"Customer\": first(col(\"Customer\")),\n",
    "#     \"Terms\": first(col(\"Terms\")),\n",
    "#     \"Currency\": first(col(\"Currency\")),\n",
    "#     \"Gross Amount\": min(col(\"Gross Amount\")),\n",
    "#     \"Payment\": max(col(\"Payment\")),\n",
    "#     \"Open Balance\": max(col(\"Open Balance\")),\n",
    "#     \"Transaction Create Date\": max(col(\"Transaction Create Date\")),\n",
    "#     \"Document Date\": max(col(\"Document Date\")),\n",
    "#     \"Due Date\": max(col(\"Due Date\")),\n",
    "#     \"Days Open\": max(col(\"Days Open\")),\n",
    "#     \"Service Dates\": max(col(\"Service Dates\")),\n",
    "#     \"Payment Receipt Date\": col(\"Payment Receipt Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51d6ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_exprs = {\n",
    "    \"Transaction Type\": 'first',  # Pick the first occurrence of \"Payment Type\"\n",
    "    \"Project ID\": 'first',  # Calculate the maximum profit\n",
    "    \"Customer\": 'first',\n",
    "    \"Terms\": 'first',\n",
    "    \"Currency\": 'first',\n",
    "    \"Gross Amount\": 'min',\n",
    "    \"Payment\": 'max',\n",
    "    \"Open Balance\": 'max',\n",
    "    \"Transaction Create Date\": 'max',\n",
    "    \"Document Date\": 'max',\n",
    "    \"Due Date\": 'max',\n",
    "    \"Days Open\": 'max',\n",
    "    \"Service Dates\": 'max',\n",
    "    \"Payment Receipt Date\": 'first'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd9cf7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first(col(\"Transaction Type\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39543ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby_df_process_2.agg(agg_exprs).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01058292",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df_result = groupby_df_process_2.agg(agg_exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19a9a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column got renamed need to look for other way, there is one commented above but causing some error\n",
    "for column in process_df_result.columns:\n",
    "#     print(column)\n",
    "    new_column = column\n",
    "    remove_list = [\"max\",\"min\",\"(\",\")\",\"first\"]\n",
    "    for char in remove_list:\n",
    "#         print(char)\n",
    "        if char in new_column:\n",
    "            new_column=new_column.replace(char,\"\")\n",
    "    process_df_result=process_df_result.withColumnRenamed(column,new_column)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fa8abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_df_result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e83b3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df_result = process_df_result.withColumn('Outstanding', lit('Cleared'))\n",
    "process_df_result=process_df_result.withColumn('Applying Link Amount', -col(\"Payment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72ccf85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------------+\n",
      "|Transaction ID|Outstanding|Applying Link Amount|\n",
      "+--------------+-----------+--------------------+\n",
      "|       3191846|    Cleared|              -84.87|\n",
      "|       3196377|    Cleared|              -243.0|\n",
      "|       3152869|    Cleared|            -1394.96|\n",
      "|       3136755|    Cleared|                -0.0|\n",
      "|       3117490|    Cleared|             -111.29|\n",
      "|       3119866|    Cleared|            -1397.43|\n",
      "|       3051990|    Cleared|             -525.98|\n",
      "|       3034098|    Cleared|              -19.48|\n",
      "|       2979232|    Cleared|              -119.6|\n",
      "|       2889570|    Cleared|            -2857.77|\n",
      "|       2840729|    Cleared|             -1000.0|\n",
      "|       3195531|    Cleared|                -0.0|\n",
      "|       3198467|    Cleared|             -446.38|\n",
      "|       3154622|    Cleared|                -0.0|\n",
      "|       3114388|    Cleared|               -0.08|\n",
      "|       3049865|    Cleared|                -0.0|\n",
      "|       3050589|    Cleared|                -0.0|\n",
      "|       3037054|    Cleared|             -185.24|\n",
      "|       3013001|    Cleared|               -0.01|\n",
      "|       2971297|    Cleared|               -0.17|\n",
      "+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_df_result.select([\"Transaction ID\", \"Outstanding\",\"Applying Link Amount\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d67c1b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Result : 14286\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Result :\", process_df_result.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "455969aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(process_df_result.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a9529",
   "metadata": {},
   "source": [
    "### Output : process_df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c832f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8af9b4b0",
   "metadata": {},
   "source": [
    "## 3. Tranasaction Type  != \"Credit Memo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fea6022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 366688 16---->366688 17\n"
     ]
    }
   ],
   "source": [
    "temp_df_process_3=sales_report_kmpg_july.filter(sales_report_kmpg_july['Transaction Type'] != 'Credit Memo')\n",
    "\n",
    "print(\"Input :\", temp_df_process_3.count(), len(temp_df_process_3.columns), end=\"---->\")\n",
    "\n",
    "temp_df_process_3 = temp_df_process_3.withColumn('Customer', upper(trim(col('Customer'))))\n",
    "temp_df_process_3 = temp_df_process_3.withColumn(\"FileName\", lit(\"saleReprtKmpg_july\"))\n",
    "print(temp_df_process_3.count(), len(temp_df_process_3.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8f098",
   "metadata": {},
   "source": [
    "# customer_payment_history \n",
    "customer_payment_history =customer_payment_history.withColumn('Date', date_format(to_date(customer_payment_history['Date'],'M/d/yyyy'),'yyyy-MM-dd' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bfbb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_payment_history.filter(col(\"payment Date\").isNull()).count()\n",
    "# customer_payment_history.filter(col(\"Payment Date\").rlike(r\"^\\d{1,2}/\\d{1,2}/\\d{4}$\")).count()\n",
    "# customer_payment_history.filter(col('Payment Date').like('%1/1/1900%')).select('Payment Date').show()\n",
    "# customer_payment_history =customer_payment_history.withColumn('Date', to_date(customer_payment_history['Date'],'M/d/yyyy'))\n",
    "# customer_payment_history =customer_payment_history.withColumn('Date', date_format(to_date(customer_payment_history['Date'],'M/d/yyyy'),'yyyy-MM-dd' ))\n",
    "# customer_payment_history =customer_payment_history.withColumn('Date', date_format(customer_payment_history['Date'], 'MM/dd/yyyy'))\n",
    "# customer_payment_history =customer_payment_history.withColumn('Date', to_date(customer_payment_history['Date'],'yyyy-MM-dd' ))\n",
    "# customer_payment_history = customer_payment_history.fillna('1/1/1900',subset=['Payment Date'])\n",
    "# customer_payment_history =customer_payment_history.withColumn('Payment Date', to_date(date_format(customer_payment_history['Payment Date'],'yyyy-MM-dd'),'M/d/yyyy' ))\n",
    "# customer_payment_history = customer_payment_history.withColumn('Document Number', upper(trim(col('Document Number'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64258882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 315677, 47, blank billing cycle : 22\n"
     ]
    }
   ],
   "source": [
    "customer_payment_history = spark.read.options(inferSchema='True').csv('csv_converted\\Customer_Payment_history_july.csv', header=True, inferSchema=True, sep=',')\n",
    "print(f\"Input {customer_payment_history.count()}, {len(customer_payment_history.columns)}, blank billing cycle : {customer_payment_history.filter(customer_payment_history['Billing Cycle'].isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b77d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 315676, columns : 48\n",
      "Output 315676, columns : 48, blank billing cycle : 21\n"
     ]
    }
   ],
   "source": [
    "customer_payment_history=customer_payment_history.withColumnRenamed('Currency0','Currency')\n",
    "customer_payment_history=customer_payment_history.withColumnRenamed('Currency24','Currency2')\n",
    "customer_payment_history=customer_payment_history.withColumnRenamed('Approved for Email10','Approved for Email')\n",
    "customer_payment_history=customer_payment_history.withColumnRenamed('Approved for Email44','Approved for Email')\n",
    "customer_payment_history =customer_payment_history.withColumn('Date', to_date(customer_payment_history['Date'],'M/d/yyyy'))\n",
    "customer_payment_history =customer_payment_history.withColumn('Payment Date', to_date(customer_payment_history['Payment Date'],'M/d/yyyy'))\n",
    "customer_payment_history= customer_payment_history.withColumnRenamed(\"Payment Date\", \"Payment Date2\")\n",
    "customer_payment_history= customer_payment_history.withColumn(\"Payment Date\", customer_payment_history[\"Date\"])\n",
    "customer_payment_history = customer_payment_history.filter((customer_payment_history['Payment Date'] >= \"2018-10-01\") & (customer_payment_history['Payment Date'] <= \"2023-07-31\"))\n",
    "print(f\"Input {customer_payment_history.count()}, columns : {len(customer_payment_history.columns)}\")\n",
    "print(f\"Output {customer_payment_history.count()}, columns : {len(customer_payment_history.columns)}, blank billing cycle : {customer_payment_history.filter(customer_payment_history['Billing Cycle'].isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a23d9e",
   "metadata": {},
   "source": [
    "# Inner join output of 2 and  customer_payment_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c368e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366688 17, : 315676 48\n"
     ]
    }
   ],
   "source": [
    "print(f\"{temp_df_process_3.count()} {len(temp_df_process_3.columns)}, : {customer_payment_history.count()} {len(customer_payment_history.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa26c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1 =  customer_payment_history.withColumnRenamed(\"Currency\",\"temp\")\n",
    "df_temp_1 = df_temp_1.withColumnRenamed(\"Approved for Email\",\"temp2\")\n",
    "joined_df_2_payment_history = temp_df_process_3.join(df_temp_1, on='Document Number', how='inner')\n",
    "joined_df_2_payment_history = joined_df_2_payment_history.withColumn(\"Outstanding\", lit(\"Cleared\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba88e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 315678\n"
     ]
    }
   ],
   "source": [
    "print(len(joined_df_2_payment_history.columns), joined_df_2_payment_history.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97b4b983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df_2_payment_history.filter(joined_df_2_payment_history['Billing Cycle'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5931ff",
   "metadata": {},
   "source": [
    "# Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a93755d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_copy = temp_df.select(\"*\")\n",
    "joined_df_2_payment_history_copy = joined_df_2_payment_history.select(\"*\")\n",
    "process_df_result_copy = process_df_result.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "1ee2e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows :  14286 315678 3948\n",
      "columns :  18 65 19\n",
      "columns :  18 65 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows : \",process_df_result.count(),joined_df_2_payment_history.count(), temp_df_copy.count())\n",
    "print(\"columns : \",len(process_df_result_copy.columns), len(joined_df_2_payment_history_copy.columns),len(temp_df_copy.columns))\n",
    "print(\"columns : \",len(process_df_result.columns), len(joined_df_2_payment_history.columns),len(temp_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "3b97e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "column_names=set()\n",
    "for df in [temp_df, joined_df_2_payment_history,process_df_result]:\n",
    "    for column in df.columns:\n",
    "        column_names.add(column)\n",
    "column_names = list(column_names)\n",
    "print(len(column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e8a675f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-->65\n"
     ]
    }
   ],
   "source": [
    "print(len(temp_df_copy.columns), end= \"-->\")\n",
    "for column in column_names:\n",
    "    if column not in temp_df_copy.columns:\n",
    "#         print(column)\n",
    "        temp_df_copy = temp_df_copy.withColumn(column,lit(None).cast(\"string\"))\n",
    "print(len(temp_df_copy.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "e37f9b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65-->65\n"
     ]
    }
   ],
   "source": [
    "print(len(joined_df_2_payment_history_copy.columns), end= \"-->\")\n",
    "for column in column_names:\n",
    "    if column not in joined_df_2_payment_history_copy.columns:\n",
    "        print(column)\n",
    "        joined_df_2_payment_history_copy = joined_df_2_payment_history_copy.withColumn(column,lit(None).cast(\"string\"))\n",
    "print(len(joined_df_2_payment_history_copy.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6d5d1938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-->65\n"
     ]
    }
   ],
   "source": [
    "print(len(process_df_result_copy.columns), end= \"-->\")\n",
    "for column in column_names:\n",
    "    if column not in process_df_result_copy.columns:\n",
    "#         print(column)\n",
    "        process_df_result_copy = process_df_result_copy.withColumn(column,lit(None).cast(\"string\"))\n",
    "print(len(process_df_result_copy.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "c8ad60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = temp_df_copy.unionByName(joined_df_2_payment_history_copy).unionByName(process_df_result_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "55fb63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333912 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43992937.400000066"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(union_df.count(), len(union_df.columns))\n",
    "union_df.select(sum(col('Open Balance'))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "db9a529d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18255"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_df.filter(union_df[\"Billing Cycle\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ac2ae0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333767"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_df.filter(col(\"Customer\").like(r\"% %\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "97ceb10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_df.filter(col(\"Customer\").rlike(r\"^\\s\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "38aeacd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Customer|\n",
      "+--------------------+\n",
      "|KIMA : Katy Inter...|\n",
      "|MDSTR : Medstar D...|\n",
      "| LEXC : TS - Urology|\n",
      "|DHHA : Denver Hea...|\n",
      "|MSHS : Mount Sina...|\n",
      "|UABHS : Emergency...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|DUKE : North Caro...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|USACS : Woodland ...|\n",
      "|PROVHS : Gastroen...|\n",
      "|CORNELLU : Weill ...|\n",
      "|AHMG : Atrium Hea...|\n",
      "|MLHC : Main Line ...|\n",
      "|DHC : Dartmouth-H...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df.select(\"Customer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "ca2ee824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Customer|\n",
      "+--------------------+\n",
      "|KIMA : Katy Inter...|\n",
      "|MDSTR : Medstar D...|\n",
      "| LEXC : TS - Urology|\n",
      "|DHHA : Denver Hea...|\n",
      "|MSHS : Mount Sina...|\n",
      "|UABHS : Emergency...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|DUKE : North Caro...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|USACS : Woodland ...|\n",
      "|PROVHS : Gastroen...|\n",
      "|CORNELLU : Weill ...|\n",
      "|AHMG : Atrium Hea...|\n",
      "|MLHC : Main Line ...|\n",
      "|DHC : Dartmouth-H...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df.filter(col(\"Customer\").like(\"% %\")).select(\"Customer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ce23b",
   "metadata": {},
   "source": [
    "<span style = \"color:red\"><b>Note  : </b></span>\n",
    "- Observed white spaces in the data may be causing error in joining need to verify or rectify "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd3018",
   "metadata": {},
   "source": [
    "## Adding column Billing Cycle2 - Contract Terms per Client (1) - Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a8de0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanining data and creating csv file to avoid multiline issue\n",
    "try:\n",
    "    filepath = \"D:/Bluethink/altryx/AL/Contract Terms per Client (1).xlsx\"\n",
    "    filename = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    pandas_df = pd.read_excel(filepath)\n",
    "    df= pandas_df\n",
    "    for column in df.columns:\n",
    "#         print(column, \"datatype : \",pandas_df[column].dtypes, end=\" : -->\")\n",
    "        datatype = df[column].dtypes\n",
    "        df[column] = df[column].astype(str)\n",
    "#         print(df[column].dtypes, end=\" -->\")\n",
    "        df.fillna(\"\", inplace=True)\n",
    "        df[column] = df[column].str.replace('\\n', ' ')\n",
    "        df[column] = df[column].astype(datatype)\n",
    "#         print(df[column].dtypes)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    new_file = \"csv_converted/temp\"+\"/\" +filename+\".csv\"\n",
    "    df.to_csv(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "bb924867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pandas_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e6b7cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Rows 7986 columns 2 billing blanks 154\n",
      "Inputs Rows 333912 columns 65 billing blanks 18255\n",
      "333912 66\n"
     ]
    }
   ],
   "source": [
    "contract_terms_df=spark.read.options(inferSchema='True').options(multiline='True').csv(new_file, header=True, sep= \",\")\n",
    "contract_terms_df = contract_terms_df.drop('_c0')\n",
    "contract_terms_df= contract_terms_df.withColumnRenamed(\"Name\",\"Customer\")\n",
    "contract_terms_df = contract_terms_df.select([\"Customer\",\"Billing Cycle\"])\n",
    "contract_terms_df = contract_terms_df.withColumnRenamed(\"Billing Cycle\",\"Billing Cycle2\")\n",
    "\n",
    "print(f\"Inputs Rows {contract_terms_df.count()} columns {len(contract_terms_df.columns)} billing blanks {contract_terms_df.filter(contract_terms_df['Billing Cycle2'].isNull()).count()}\")\n",
    "print(f\"Inputs Rows {union_df.count()} columns {len(union_df.columns)} billing blanks {union_df.filter(union_df['Billing Cycle'].isNull()).count()}\")\n",
    "df_block1 = union_df.join(contract_terms_df,on='Customer', how='leftouter')\n",
    "print(df_block1.count(),len(df_block1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_terms_df.dropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "5fad3876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Customer|\n",
      "+--------------------+\n",
      "|                21CO|\n",
      "|21CO : Gulfstream...|\n",
      "|21CO : Regional B...|\n",
      "|     21CO : Uromedix|\n",
      "|                 AAI|\n",
      "|                AAMD|\n",
      "|                 AAS|\n",
      "|AAS : Arizona Ass...|\n",
      "|                ABMH|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "|ABMH : Abington M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contract_terms_df.select(\"Customer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8daecd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Customer|\n",
      "+--------------------+\n",
      "|KIMA : Katy Inter...|\n",
      "|MDSTR : Medstar D...|\n",
      "| LEXC : TS - Urology|\n",
      "|DHHA : Denver Hea...|\n",
      "|MSHS : Mount Sina...|\n",
      "|UABHS : Emergency...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|DUKE : North Caro...|\n",
      "|MSHS : Mount Sina...|\n",
      "|MSHS : Mount Sina...|\n",
      "|USACS : Woodland ...|\n",
      "|PROVHS : Gastroen...|\n",
      "|CORNELLU : Weill ...|\n",
      "|AHMG : Atrium Hea...|\n",
      "|MLHC : Main Line ...|\n",
      "|DHC : Dartmouth-H...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df.select(\"Customer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5793130e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324075"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_block1.filter(df_block1[\"Billing Cycle2\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "db1f30f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contract_terms_df.filter(contract_terms_df[\"Billing Cycle2\"]==\" \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bdec511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Customer': 1, 'Billing Cycle2': 1}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dub_column(contract_terms_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63576d",
   "metadata": {},
   "source": [
    "# Adding Column region2 - join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b91b0587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"D:\\Bluethink\\alytrx project\\csv_converted\\Region (2).csv\"\n",
    "region_df=spark.read.options(inferSchema='True').csv('csv_converted\\Region (2).csv', header=True, inferSchema=True, sep=',')\n",
    "region_df_join_operation = region_df.select([\"Name\",\"region\"]) # dataframe to perform join operation\n",
    "region_df_join_operation = region_df_join_operation.withColumnRenamed(\"Name\",\"Customer\")\n",
    "region_df_join_operation = region_df_join_operation.withColumnRenamed(\"region\",\"region2\")\n",
    "region_df = region_df.withColumnRenamed(\"Internal ID0\",\"Internal ID\")\n",
    "region_df = region_df.withColumn('Name',upper(trim(col('Name'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c057473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11464-->11462\n"
     ]
    }
   ],
   "source": [
    "# removing row having same customer name and null in region\n",
    "extra_column_to_remove=region_df_join_operation.groupBy(\"Customer\").count().filter(col(\"count\")>1).select(\"Customer\")\n",
    "print(region_df_join_operation.count(), end=\"-->\")\n",
    "for row in extra_column_to_remove.select(\"Customer\").collect():\n",
    "    for data in row:\n",
    "        region_df_join_operation=region_df_join_operation.filter((region_df_join_operation[\"Customer\"]!=data) | (region_df_join_operation[\"region2\"].isNotNull()))\n",
    "print(region_df_join_operation.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a9d755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block1 = df_block1.join(region_df_join_operation, on=\"Customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26a77fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333912 67\n"
     ]
    }
   ],
   "source": [
    "print(df_block1.count(), len(df_block1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c8175",
   "metadata": {},
   "source": [
    "## Adding Column `Exclude` from `Bankrupt and Other Customers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c7c7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankrupt_df=spark.read.options(inferSchema='True').csv('csv_converted/Bankrupt and Other Customers (1)_sheet1.csv', header=True, inferSchema=True, sep=',')\n",
    "bankrupt_df = bankrupt_df.withColumn(\"Customer Name\", trim(col(\"Customer Name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f765ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_block1.join(bankrupt_df, df_block1[\"Customer\"]==bankrupt_df[\"Customer Name\"], how=\"left\").drop(\"Customer Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ea9eeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_block1.filter(df_block1[\"Billing Cycle\"]==\" \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56e597ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324128"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_block1.filter(df_block1[\"Billing Cycle2\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d24ef4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18255"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_block1.filter(df_block1[\"Billing Cycle\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4def810c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324128"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filter(df_block1[\"Billing Cycle2\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ea9e63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18255"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filter(df_block1[\"Billing Cycle\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da24056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fc970bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_block1.withColumn(\n",
    "        \"Billing Cycle2\",\n",
    "        when(\n",
    "            (col(\"Billing Cycle2\").isNull()) |\n",
    "            (col(\"Billing Cycle2\") == \"\") |\n",
    "            (col(\"Billing Cycle2\") == \" \"),\n",
    "            col(\"Billing Cycle\")\n",
    "        ).otherwise(col(\"Billing Cycle2\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fabc2798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324128"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filter(a[\"Billing Cycle2\"].isNull()).select(\"Billing Cycle2\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c244965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333912 68\n"
     ]
    }
   ],
   "source": [
    "print(a.count(), len(a.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37554db5",
   "metadata": {},
   "source": [
    "<!-- import time\n",
    "completed = 0\n",
    "start_time = time.time()\n",
    "for i, row in enumerate(region_df.rdd.collect()):\n",
    "    if (i+1)%100==0:\n",
    "        print(i+1, end=\"\")\n",
    "        completed += 0.8724480893386843\n",
    "        print(f\"  : {completed:.2f} %\", end = \" ,\")\n",
    "        print(f\"Took {(time.time()-start_time):.2f} seconds\")\n",
    "#         \n",
    "#         print(region_df.count)\n",
    "    else:\n",
    "        if i%2==0:\n",
    "            print(\".\", end=\"\")\n",
    "    c = c.withColumn(\"region2\", when(c[\"Customer\"] == row[\"Name\"], row[\"region2\"]).otherwise(None))\n",
    "                                                                     \n",
    "     -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ddb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
